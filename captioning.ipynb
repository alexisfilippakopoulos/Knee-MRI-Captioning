{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install nltk rouge-score\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt', force=True)\n",
        "print(nltk.data.path)\n",
        "\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import clip\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RClsaeEctkAz",
        "outputId": "0eeb6441-827a-4320-f581-ae8f02fe5cd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/root/nltk_data', '/usr/nltk_data', '/usr/share/nltk_data', '/usr/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.11/dist-packages (2.5.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (3.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2.0.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-9ehk6v46\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-9ehk6v46\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_caption(row):\n",
        "    if row['abnormal'] == 0:\n",
        "        return \"Healthy knee\"\n",
        "\n",
        "    findings = []\n",
        "    if row['acl'] == 1:\n",
        "        findings.append(\"ACL tear\")\n",
        "    if row['meniscus'] == 1:\n",
        "        findings.append(\"Meniscus tear\")\n",
        "    if findings:\n",
        "        return \" and a \".join(findings) + \".\"\n",
        "    else:\n",
        "        return \"Unspecified abnormality.\"\n",
        "\n",
        "\n",
        "class ClipCaptionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A neural network model that combines CLIP embeddings with GPT-2 for image caption generation.\n",
        "\n",
        "    The model projects the CLIP image embeddings into a prefix representation that is then concatenated\n",
        "    with the input captions. It uses the GPT-2 model to generate captions conditioned on the image embeddings.\n",
        "\n",
        "    Args:\n",
        "        clip_dim (int, optional): The dimensionality of the CLIP image embeddings. Default is 512.\n",
        "        prefix_len (int, optional): The number of tokens in the prefix generated from the image embedding.\n",
        "                                    This prefix is concatenated with the input captions. Default is 10.\n",
        "\n",
        "    Forward Method:\n",
        "        The forward pass takes the CLIP image embedding, the caption tokens, and the attention mask as inputs.\n",
        "        It generates the image-conditioned caption by combining the image embedding prefix with the caption\n",
        "        embeddings and feeding them through the GPT-2 model.\n",
        "\n",
        "    Args:\n",
        "        image_embedding (torch.Tensor): A tensor of shape (batch_size, clip_dim) representing the image embeddings from CLIP.\n",
        "        captions (torch.Tensor): A tensor of shape (batch_size, caption_len) representing the tokenized captions.\n",
        "        attention_mask (torch.Tensor): A tensor of shape (batch_size, caption_len) indicating the padding positions in captions.\n",
        "\n",
        "    Returns:\n",
        "        transformers.modeling_outputs.CausalLMOutputWithCrossAttentions: The output of the GPT-2 model,\n",
        "        containing the logits and additional information.\n",
        "    \"\"\"\n",
        "    def __init__(self, clip_dim=512, prefix_len=10):\n",
        "        super().__init__()\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "        self.prefix_len = prefix_len\n",
        "        self.clip_project = nn.Linear(clip_dim, self.gpt.config.n_embd * prefix_len)\n",
        "\n",
        "    def forward(self, image_embedding, captions, attention_mask):\n",
        "        batch_size = captions.shape[0]\n",
        "\n",
        "        image_embedding = image_embedding.float()\n",
        "\n",
        "        prefix_embedding = self.clip_project(image_embedding).view(batch_size, self.prefix_len, -1)\n",
        "        caption_embeddings = self.gpt.transformer.wte(captions)\n",
        "\n",
        "        embeddings = torch.cat((prefix_embedding, caption_embeddings), dim=1)\n",
        "\n",
        "        extended_attention = torch.cat((\n",
        "            torch.ones((batch_size, self.prefix_len), device=attention_mask.device),\n",
        "            attention_mask\n",
        "        ), dim=1)\n",
        "\n",
        "        labels = torch.cat((\n",
        "            torch.full((batch_size, self.prefix_len), -100, device=captions.device),\n",
        "            captions\n",
        "        ), dim=1)\n",
        "\n",
        "        outputs = self.gpt(inputs_embeds=embeddings, attention_mask=extended_attention, labels=labels)\n",
        "        return outputs\n",
        "\n",
        "class MRICaptionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom dataset for loading MRI scans and their corresponding captions, suitable for image captioning tasks.\n",
        "\n",
        "    This dataset assumes the MRI scans are stored as numpy arrays in a specified directory, with each array representing\n",
        "    a 3D volume of slices. For each MRI scan, the dataset loads a representative slice, preprocesses it, and tokenizes\n",
        "    the associated caption. The caption is tokenized and padded/truncated to a specified length.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pandas.DataFrame): A dataframe containing the metadata for the MRI scans. The dataframe should\n",
        "                                      contain columns 'exam' (MRI scan identifier) and 'caption' (text description of the scan).\n",
        "        image_dir (str): The directory containing the MRI scan files (in `.npy` format). Each file is named by its exam ID.\n",
        "        transform (callable): A transformation function to be applied to the image (e.g., resizing, normalization).\n",
        "        tokenizer (transformers.PreTrainedTokenizer): A tokenizer to tokenize the captions, typically from HuggingFace's `transformers` library.\n",
        "        max_length (int, optional): The maximum length for tokenizing captions. Default is 50 tokens.\n",
        "        num_slices_to_use (int, optional): The number of slices to select from the MRI scan volume. Default is 5 slices.\n",
        "\n",
        "    Attributes:\n",
        "        clip_dim (int): The dimension of the CLIP embedding (set to 512).\n",
        "\n",
        "    Methods:\n",
        "        __len__(): Returns the number of items in the dataset.\n",
        "        __getitem__(idx): Loads the MRI scan and its corresponding caption at the specified index `idx`, preprocesses the slice,\n",
        "                          and returns the image tensor, tokenized caption (input IDs and attention mask), the exam ID, and the selected slice indices.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - img_tensor (torch.Tensor): The transformed image tensor for the representative slice.\n",
        "            - input_ids (torch.Tensor): The tokenized caption's input IDs.\n",
        "            - attention_mask (torch.Tensor): The attention mask for the tokenized caption.\n",
        "            - exam_id (str): The MRI scan's exam ID.\n",
        "            - selected_slices (list): A list of the indices of the selected slices from the MRI scan.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataframe, image_dir, transform, tokenizer, max_length=50, num_slices_to_use=5):\n",
        "        self.data = dataframe\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.num_slices_to_use = num_slices_to_use\n",
        "        self.clip_dim = 512\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        exam_id = str(row['exam']).zfill(4)\n",
        "        caption = row['caption']\n",
        "        img_path = os.path.join(self.image_dir, f\"{exam_id}.npy\")\n",
        "\n",
        "        scan = np.load(img_path)\n",
        "        num_slices = scan.shape[0]\n",
        "\n",
        "        # select evenly spaced slices\n",
        "        if num_slices <= self.num_slices_to_use:\n",
        "            selected_slices = range(num_slices)\n",
        "        else:\n",
        "            selected_slices = np.linspace(0, num_slices-1, self.num_slices_to_use, dtype=int)\n",
        "\n",
        "        # take the middle slice for dataset output (representative slice)\n",
        "        mid_slice = num_slices // 2\n",
        "        slice_img = scan[mid_slice]\n",
        "        slice_img = ((slice_img - slice_img.min()) / (slice_img.max() - slice_img.min()) * 255).astype(np.uint8)\n",
        "        slice_rgb = np.stack([slice_img, slice_img, slice_img], axis=-1)\n",
        "        pil_img = Image.fromarray(slice_rgb)\n",
        "        img_tensor = self.transform(pil_img)\n",
        "\n",
        "        tokens = self.tokenizer(\n",
        "            caption,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids = tokens.input_ids.squeeze(0)\n",
        "        attention_mask = tokens.attention_mask.squeeze(0)\n",
        "\n",
        "        return img_tensor, input_ids, attention_mask, exam_id, selected_slices.tolist() if hasattr(selected_slices, 'tolist') else list(selected_slices)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    A collate function to combine a list of samples into a batch for the DataLoader.\n",
        "\n",
        "    This function takes a batch of data, which is a list of tuples returned by the dataset's `__getitem__` method.\n",
        "    It stacks the image tensors, tokenized input IDs, and attention masks along the batch dimension to form a single\n",
        "    batch tensor. The exam IDs and selected slice indices are returned as lists without modification.\n",
        "\n",
        "    Args:\n",
        "        batch (list of tuples): A list where each element is a tuple containing:\n",
        "            - images (torch.Tensor): The transformed image tensors for each sample.\n",
        "            - input_ids (torch.Tensor): The tokenized captions' input IDs.\n",
        "            - attention_masks (torch.Tensor): The attention masks for the tokenized captions.\n",
        "            - exam_ids (str): The MRI scan exam ID.\n",
        "            - selected_slices (list): A list of indices for the selected slices in the MRI scan.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - images_tensor (torch.Tensor): A tensor of shape (batch_size, C, H, W) representing the stacked image tensors.\n",
        "            - input_ids_tensor (torch.Tensor): A tensor of shape (batch_size, max_length) representing the stacked input IDs.\n",
        "            - attention_masks_tensor (torch.Tensor): A tensor of shape (batch_size, max_length) representing the stacked attention masks.\n",
        "            - exam_ids (tuple): A tuple of exam IDs for each sample in the batch.\n",
        "            - selected_slices (tuple): A tuple of lists containing the selected slice indices for each sample in the batch.\n",
        "    \"\"\"\n",
        "    images, input_ids, attention_masks, exam_ids, selected_slices = zip(*batch)\n",
        "    images_tensor = torch.stack(images)\n",
        "    input_ids_tensor = torch.stack(input_ids)\n",
        "    attention_masks_tensor = torch.stack(attention_masks)\n",
        "\n",
        "    return images_tensor, input_ids_tensor, attention_masks_tensor, exam_ids, selected_slices\n",
        "\n",
        "def encode_selected_slices(clip_model, exam_id, selected_slices, image_dir, transform, device):\n",
        "    \"\"\"\n",
        "    Encodes selected MRI scan slices using a CLIP model and returns the mean embedding of the selected slices.\n",
        "\n",
        "    This function loads an MRI scan for a given `exam_id`, selects specific slices based on the provided indices,\n",
        "    preprocesses each slice to a 3-channel image, and computes their embeddings using the provided CLIP model.\n",
        "    It then averages the embeddings of the selected slices to produce a single mean embedding representing the scan.\n",
        "\n",
        "    Args:\n",
        "        clip_model (CLIPModel): A pre-trained CLIP model for encoding images into embeddings.\n",
        "        exam_id (str): The exam ID (MRI scan identifier) to locate the corresponding scan in the `image_dir`.\n",
        "        selected_slices (list of int): A list of indices specifying which slices of the scan to use for embedding.\n",
        "        image_dir (str): The directory containing the MRI scan files, which are expected to be in `.npy` format.\n",
        "        transform (callable): A transformation function to preprocess the slices (e.g., resizing, normalization).\n",
        "        device (torch.device): The device (CPU or GPU) on which the CLIP model and the tensors should be processed.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: A tensor representing the mean embedding of the selected MRI slices. The tensor shape is (1, embedding_dim).\n",
        "    \"\"\"\n",
        "    img_path = os.path.join(image_dir, f\"{exam_id}.npy\")\n",
        "    scan = np.load(img_path)\n",
        "\n",
        "    slice_embeddings = []\n",
        "    for slice_idx in selected_slices:\n",
        "        # convert slice to 3-channel image\n",
        "        slice_img = scan[slice_idx]\n",
        "        slice_img = ((slice_img - slice_img.min()) / (slice_img.max() - slice_img.min()) * 255).astype(np.uint8)\n",
        "        slice_rgb = np.stack([slice_img, slice_img, slice_img], axis=-1)\n",
        "        pil_img = Image.fromarray(slice_rgb)\n",
        "        img_tensor = transform(pil_img).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding = clip_model.encode_image(img_tensor)\n",
        "        slice_embeddings.append(embedding)\n",
        "\n",
        "    all_embeddings = torch.cat(slice_embeddings, dim=0)\n",
        "    mean_embedding = torch.mean(all_embeddings, dim=0, keepdim=True)\n",
        "\n",
        "    return mean_embedding\n",
        "\n",
        "def train_epoch(caption_model, clip_model, dataloader, optimizer, device, image_dir, transform):\n",
        "    \"\"\"\n",
        "    Trains the captioning model for one epoch on the provided dataloader.\n",
        "\n",
        "    This function runs through one full pass of the dataset, computing the embeddings for each MRI scan slice using\n",
        "    the provided CLIP model, and then computes the loss and updates the parameters of the captioning model.\n",
        "\n",
        "    Args:\n",
        "        caption_model (nn.Module): The captioning model to be trained (e.g., a model combining CLIP and GPT-2).\n",
        "        clip_model (CLIPModel): A pre-trained CLIP model used to encode the MRI scan slices into embeddings.\n",
        "        dataloader (DataLoader): A PyTorch DataLoader that provides batches of data for training.\n",
        "        optimizer (torch.optim.Optimizer): The optimizer used for model parameter updates (e.g., Adam).\n",
        "        device (torch.device): The device (CPU or GPU) where the models and tensors are processed.\n",
        "        image_dir (str): The directory containing the MRI scan files in `.npy` format.\n",
        "        transform (callable): A transformation function applied to each MRI slice before encoding.\n",
        "\n",
        "    Returns:\n",
        "        float: The average loss over the entire epoch.\n",
        "    \"\"\"\n",
        "    caption_model.train()\n",
        "    total_loss = 0\n",
        "    batch_count = 0\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
        "        images, input_ids, attention_mask, exam_ids, selected_slices = batch\n",
        "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
        "\n",
        "        batch_embeddings = []\n",
        "        for i in range(len(exam_ids)):\n",
        "            exam_embedding = encode_selected_slices(\n",
        "                clip_model,\n",
        "                exam_ids[i],\n",
        "                selected_slices[i],\n",
        "                image_dir,\n",
        "                transform,\n",
        "                device\n",
        "            )\n",
        "            batch_embeddings.append(exam_embedding)\n",
        "\n",
        "        image_embeddings = torch.cat(batch_embeddings, dim=0)\n",
        "\n",
        "        outputs = caption_model(image_embeddings, input_ids, attention_mask)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate(caption_model, clip_model, dataloader, device, tokenizer, image_dir, transform):\n",
        "    caption_model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    true_caption_counts = {}\n",
        "    pred_caption_counts = {}\n",
        "    caption_pairs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
        "            images, input_ids, attention_mask, exam_ids, selected_slices = batch\n",
        "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
        "\n",
        "            batch_embeddings = []\n",
        "            for i in range(len(exam_ids)):\n",
        "                exam_embedding = encode_selected_slices(\n",
        "                    clip_model,\n",
        "                    exam_ids[i],\n",
        "                    selected_slices[i],\n",
        "                    image_dir,\n",
        "                    transform,\n",
        "                    device\n",
        "                )\n",
        "                batch_embeddings.append(exam_embedding)\n",
        "\n",
        "            image_embeddings = torch.cat(batch_embeddings, dim=0)\n",
        "\n",
        "            outputs = caption_model(image_embeddings, input_ids, attention_mask)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            batch_size = input_ids.size(0)\n",
        "            for i in range(batch_size):\n",
        "                true_caption = tokenizer.decode(input_ids[i], skip_special_tokens=True)\n",
        "\n",
        "                true_caption_counts[true_caption] = true_caption_counts.get(true_caption, 0) + 1\n",
        "\n",
        "                prefix_embed = caption_model.clip_project(image_embeddings[i].float().unsqueeze(0)) \\\n",
        "                    .view(1, caption_model.prefix_len, -1)\n",
        "\n",
        "                attention_prefix = torch.ones(1, caption_model.prefix_len, device=device)\n",
        "\n",
        "                generated = caption_model.gpt.generate(\n",
        "                    inputs_embeds=prefix_embed,\n",
        "                    max_length=50,\n",
        "                    num_beams=5,\n",
        "                    early_stopping=True,\n",
        "                    pad_token_id=tokenizer.eos_token_id,\n",
        "                    attention_mask=attention_prefix\n",
        "                )\n",
        "                pred_caption = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "                pred_caption_counts[pred_caption] = pred_caption_counts.get(pred_caption, 0) + 1\n",
        "\n",
        "                caption_pairs.append((true_caption, pred_caption))\n",
        "\n",
        "    # Calculate classification metrics\n",
        "    classification_metrics = calculate_classification_metrics(caption_pairs)\n",
        "\n",
        "    avg_loss = total_loss / max(1, len(dataloader))\n",
        "\n",
        "    total_samples = len(caption_pairs)\n",
        "    unique_pred_captions = len(pred_caption_counts)\n",
        "    unique_true_captions = len(true_caption_counts)\n",
        "\n",
        "    pred_to_true_map = {}\n",
        "    for true_cap, pred_cap in caption_pairs:\n",
        "        if pred_cap not in pred_to_true_map:\n",
        "            pred_to_true_map[pred_cap] = {}\n",
        "        pred_to_true_map[pred_cap][true_cap] = pred_to_true_map[pred_cap].get(true_cap, 0) + 1\n",
        "\n",
        "    sorted_pred_captions = sorted(pred_caption_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    sorted_true_captions = sorted(true_caption_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    caption_analysis = {\n",
        "        \"total_samples\": total_samples,\n",
        "        \"unique_predicted_captions\": unique_pred_captions,\n",
        "        \"unique_true_captions\": unique_true_captions,\n",
        "        \"top_predicted_captions\": sorted_pred_captions[:10],\n",
        "        \"top_true_captions\": sorted_true_captions[:10],\n",
        "        \"prediction_to_true_map\": pred_to_true_map,\n",
        "        \"repetition_rate\": 1 - (unique_pred_captions / total_samples)\n",
        "    }\n",
        "\n",
        "    # Print all results\n",
        "    print(\"=== BASIC METRICS ===\")\n",
        "    for key, value in caption_analysis.items():\n",
        "        if key not in [\"prediction_to_true_map\"]:  # Skip the large map for cleaner output\n",
        "            print(f\"{key}: {value}\")\n",
        "\n",
        "    print(f\"\\nAverage Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    print(\"\\n=== CLASSIFICATION METRICS ===\")\n",
        "    print(f\"Overall Accuracy: {classification_metrics['overall_accuracy']:.4f}\")\n",
        "    print(f\"Macro Average Precision: {classification_metrics['macro_precision']:.4f}\")\n",
        "    print(f\"Macro Average Recall: {classification_metrics['macro_recall']:.4f}\")\n",
        "    print(f\"Macro Average F1: {classification_metrics['macro_f1']:.4f}\")\n",
        "\n",
        "    return avg_loss, classification_metrics\n",
        "\n",
        "\n",
        "def calculate_classification_metrics(caption_pairs):\n",
        "    \"\"\"\n",
        "    Calculate per-caption classification metrics treating each unique caption as a class.\n",
        "\n",
        "    Args:\n",
        "        caption_pairs: List of tuples (true_caption, predicted_caption)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing overall accuracy, macro averages, and per-caption metrics\n",
        "    \"\"\"\n",
        "    from collections import defaultdict\n",
        "\n",
        "    # Get all unique captions\n",
        "    all_captions = set()\n",
        "    for true_cap, pred_cap in caption_pairs:\n",
        "        all_captions.add(true_cap)\n",
        "        all_captions.add(pred_cap)\n",
        "\n",
        "    # Initialize confusion matrix components for each caption\n",
        "    per_caption_metrics = {}\n",
        "\n",
        "    for caption in all_captions:\n",
        "        # For each caption as a \"positive\" class\n",
        "        tp = sum(1 for true_cap, pred_cap in caption_pairs if true_cap == caption and pred_cap == caption)\n",
        "        fp = sum(1 for true_cap, pred_cap in caption_pairs if true_cap != caption and pred_cap == caption)\n",
        "        fn = sum(1 for true_cap, pred_cap in caption_pairs if true_cap == caption and pred_cap != caption)\n",
        "        tn = sum(1 for true_cap, pred_cap in caption_pairs if true_cap != caption and pred_cap != caption)\n",
        "\n",
        "        # Calculate metrics for this caption\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "        accuracy = (tp + tn) / len(caption_pairs) if len(caption_pairs) > 0 else 0.0\n",
        "\n",
        "        per_caption_metrics[caption] = {\n",
        "            'tp': tp,\n",
        "            'fp': fp,\n",
        "            'fn': fn,\n",
        "            'tn': tn,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'accuracy': accuracy,\n",
        "            'support': tp + fn  # Number of true instances of this caption\n",
        "        }\n",
        "\n",
        "    # Calculate macro averages\n",
        "    macro_precision = sum(metrics['precision'] for metrics in per_caption_metrics.values()) / len(per_caption_metrics)\n",
        "    macro_recall = sum(metrics['recall'] for metrics in per_caption_metrics.values()) / len(per_caption_metrics)\n",
        "    macro_f1 = sum(metrics['f1'] for metrics in per_caption_metrics.values()) / len(per_caption_metrics)\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    correct_predictions = sum(1 for true_cap, pred_cap in caption_pairs if true_cap == pred_cap)\n",
        "    overall_accuracy = correct_predictions / len(caption_pairs) if len(caption_pairs) > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        'per_caption_metrics': per_caption_metrics,\n",
        "        'macro_precision': macro_precision,\n",
        "        'macro_recall': macro_recall,\n",
        "        'macro_f1': macro_f1,\n",
        "        'overall_accuracy': overall_accuracy,\n",
        "        'total_caption_classes': len(all_captions)\n",
        "    }\n",
        "\n",
        "\n",
        "def init_model(device):\n",
        "    \"\"\"\n",
        "    Initializes and returns the components required for training or inference, including a CLIP model,\n",
        "    GPT-2 tokenizer, a captioning model, and an optimizer.\n",
        "\n",
        "    This function loads the pre-trained CLIP model (ViT-B/32), freezes its parameters, and loads the GPT-2 tokenizer.\n",
        "    It also initializes the `ClipCaptionModel` for generating captions based on CLIP embeddings and sets up the\n",
        "    optimizer for the caption model.\n",
        "\n",
        "    Args:\n",
        "        device (torch.device): The device (CPU or GPU) to load the models and tensors onto.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - clip_model (CLIPModel): The pre-trained CLIP model used for image embedding extraction.\n",
        "            - preprocess (callable): The preprocessing function associated with the CLIP model.\n",
        "            - tokenizer (GPT2Tokenizer): The tokenizer for the GPT-2 model used for caption tokenization.\n",
        "            - caption_model (ClipCaptionModel): The model that generates captions from image embeddings.\n",
        "            - optimizer (torch.optim.AdamW): The optimizer for training the caption model.\n",
        "    \"\"\"\n",
        "    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "    for param in clip_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    caption_model = ClipCaptionModel(clip_dim=512, prefix_len=10).to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        caption_model.parameters(),\n",
        "        lr=5e-5,\n",
        "        weight_decay=0.01\n",
        "    )\n",
        "\n",
        "    return clip_model, preprocess, tokenizer, caption_model, optimizer\n",
        "\n",
        "def train_model(train_df, val_df, num_epochs=25, batch_size=8):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    clip_model, preprocess, tokenizer, caption_model, optimizer = init_model(device)\n",
        "\n",
        "    #train_df, val_df = train_test_split(df_balanced, test_size=0.1, random_state=42, stratify=df_balanced['caption'])\n",
        "\n",
        "    train_dataset = MRICaptionDataset(train_df, image_dir, preprocess, tokenizer, num_slices_to_use=10)\n",
        "    val_dataset = MRICaptionDataset(val_df, val_image_dir, preprocess, tokenizer, num_slices_to_use=10)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "    best_val_loss = 100\n",
        "    metrics = {'macro_precision': [],\n",
        "        'macro_recall': [],\n",
        "        'macro_f1': [],\n",
        "        'overall_accuracy':[],\n",
        "        'train_losses': [],\n",
        "        'val_losses': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        avg_train_loss = train_epoch(caption_model, clip_model, train_loader, optimizer, device, image_dir, preprocess)\n",
        "        avg_val_loss, classification_metrics = evaluate(\n",
        "            caption_model, clip_model, val_loader, device, tokenizer, val_image_dir, preprocess\n",
        "        )\n",
        "\n",
        "        print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(caption_model.state_dict(), f\"/content/drive/MyDrive/biodata Project/best_model_sagittal.pt\")\n",
        "\n",
        "        metrics['macro_precision'].append(classification_metrics['macro_precision'])\n",
        "        metrics['macro_recall'].append(classification_metrics['macro_recall'])\n",
        "        metrics['macro_f1'].append(classification_metrics['macro_f1'])\n",
        "        metrics['overall_accuracy'].append(classification_metrics['overall_accuracy'])\n",
        "        metrics['train_losses'].append(avg_train_loss)\n",
        "        metrics['val_losses'].append(avg_val_loss)\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "2zfE1sLi8NTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/drive/MyDrive/biodata Project/MRNet-v1.0\"\n",
        "plane = \"axial\"  # can be 'axial', 'coronal', 'sagittal'\n",
        "image_dir = os.path.join(base_dir, \"train\", plane)\n",
        "val_image_dir = os.path.join(base_dir, \"valid\", plane)\n",
        "\n",
        "\n",
        "abnormal_df_train = pd.read_csv(os.path.join(base_dir, \"train-abnormal.csv\"))\n",
        "acl_df_train = pd.read_csv(os.path.join(base_dir, \"train-acl.csv\"))\n",
        "meniscus_df_train = pd.read_csv(os.path.join(base_dir, \"train-meniscus.csv\"))\n",
        "\n",
        "abnormal_df_train.columns = ['exam', 'abnormal']\n",
        "acl_df_train.columns = ['exam', 'acl']\n",
        "meniscus_df_train.columns = ['exam', 'meniscus']\n",
        "\n",
        "train_df = abnormal_df_train.merge(acl_df_train, on='exam').merge(meniscus_df_train, on='exam')\n",
        "\n",
        "\n",
        "train_df['caption'] = train_df.apply(generate_caption, axis=1)\n",
        "\n",
        "\n",
        "acl_meniscus_mask = (train_df['acl'] == 1) & (train_df['meniscus'] == 1)\n",
        "acl_only_mask     = (train_df['acl'] == 1) & (train_df['meniscus'] == 0)\n",
        "meniscus_only_mask = (train_df['acl'] == 0) & (train_df['meniscus'] == 1)\n",
        "healthy_mask      = (train_df['abnormal'] == 0)\n",
        "unspecified_mask  = (train_df['abnormal'] == 1) & (train_df['acl'] == 0) & (train_df['meniscus'] == 0)\n",
        "\n",
        "# Sample 83 from each group\n",
        "df_acl_meniscus = train_df[acl_meniscus_mask].sample(n=83, random_state=42)\n",
        "df_acl_only     = train_df[acl_only_mask].sample(n=83, random_state=42)\n",
        "df_meniscus     = train_df[meniscus_only_mask].sample(n=83, random_state=42)\n",
        "df_healthy      = train_df[healthy_mask].sample(n=83, random_state=42)\n",
        "df_unspecified  = train_df[unspecified_mask].sample(n=83, random_state=42)\n",
        "\n",
        "# Concatenate them\n",
        "train_df_balanced = pd.concat([\n",
        "    df_acl_meniscus,\n",
        "    df_acl_only,\n",
        "    df_meniscus,\n",
        "    df_healthy,\n",
        "    df_unspecified\n",
        "], ignore_index=True).sample(frac=1, random_state=42)\n",
        "\n",
        "\n",
        "clip_preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "                         std=(0.26862954, 0.26130258, 0.27577711))\n",
        "])"
      ],
      "metadata": {
        "id": "fmVbJLpOiPm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"caption\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "SOZGGzvvsb4L",
        "outputId": "9d77e7a7-30a1-49c2-c8b3-e3630ca52c43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "caption\n",
              "Unspecified abnormality.         432\n",
              "Meniscus tear.                   272\n",
              "Healthy knee                     217\n",
              "ACL tear and a Meniscus tear.    125\n",
              "ACL tear.                         83\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>caption</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Unspecified abnormality.</th>\n",
              "      <td>432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Meniscus tear.</th>\n",
              "      <td>272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Healthy knee</th>\n",
              "      <td>217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ACL tear and a Meniscus tear.</th>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ACL tear.</th>\n",
              "      <td>83</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_balanced[\"caption\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "JS6ZdeHwzCnO",
        "outputId": "b87da1c5-673c-4a12-8224-9d99c478ed50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "caption\n",
              "ACL tear and a Meniscus tear.    83\n",
              "ACL tear.                        83\n",
              "Meniscus tear.                   83\n",
              "Unspecified abnormality.         83\n",
              "Healthy knee                     83\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>caption</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ACL tear and a Meniscus tear.</th>\n",
              "      <td>83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ACL tear.</th>\n",
              "      <td>83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Meniscus tear.</th>\n",
              "      <td>83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Unspecified abnormality.</th>\n",
              "      <td>83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Healthy knee</th>\n",
              "      <td>83</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abnormal_df_val = pd.read_csv(os.path.join(base_dir, \"valid-abnormal.csv\"))\n",
        "acl_df_val = pd.read_csv(os.path.join(base_dir, \"valid-acl.csv\"))\n",
        "meniscus_df_val = pd.read_csv(os.path.join(base_dir, \"valid-meniscus.csv\"))\n",
        "\n",
        "abnormal_df_val.columns = ['exam', 'abnormal']\n",
        "acl_df_val.columns = ['exam', 'acl']\n",
        "meniscus_df_val.columns = ['exam', 'meniscus']\n",
        "\n",
        "val_df = abnormal_df_val.merge(acl_df_val, on='exam').merge(meniscus_df_val, on='exam')\n",
        "\n",
        "\n",
        "val_df['caption'] = train_df.apply(generate_caption, axis=1)\n",
        "\n",
        "val_df[\"caption\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "2eky7y-6xk4y",
        "outputId": "e1481425-fcbe-40b1-f7e7-fd4a7c36d783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "caption\n",
              "Unspecified abnormality.         40\n",
              "Meniscus tear.                   40\n",
              "Healthy knee                     23\n",
              "ACL tear and a Meniscus tear.    10\n",
              "ACL tear.                         6\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>caption</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Unspecified abnormality.</th>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Meniscus tear.</th>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Healthy knee</th>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ACL tear and a Meniscus tear.</th>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ACL tear.</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = train_model(train_df_balanced, val_df, num_epochs=25, batch_size=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6qRiCxcttqL",
        "outputId": "3c92a63d-c1b4-4de3-f73a-426b05a757ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   2%|▏         | 1/52 [00:21<18:41, 22.00s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in metrics.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "KhFnh_bn0Qde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Find max values\n",
        "max_values = {metric: max(values) for metric, values in metrics.items() if metric in [\"macro_precision\", \"macro_recall\", \"macro_f1\", \"overall_accuracy\"]}\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots()\n",
        "bars = ax.bar(max_values.keys(), max_values.values(), color='skyblue')\n",
        "\n",
        "# Annotate bars with values\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax.annotate(f'{height:.3f}',\n",
        "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                xytext=(0, 0.),\n",
        "                textcoords=\"offset points\",\n",
        "                ha='center', va='bottom')\n",
        "\n",
        "ax.set_ylabel(\"Max Value\")\n",
        "ax.set_title(\"Maximum Metric Values\")\n",
        "plt.xticks(rotation=15)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"/content/axial_results.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_2fIm_1zGalD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZEj8D0cNJKTw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
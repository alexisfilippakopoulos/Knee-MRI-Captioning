{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "import clip\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(row):\n",
    "    if row['abnormal'] == 0:\n",
    "        return \"Healthy knee\"\n",
    "\n",
    "    findings = []\n",
    "    if row['acl'] == 1:\n",
    "        findings.append(\"ACL tear\")\n",
    "    if row['meniscus'] == 1:\n",
    "        findings.append(\"Meniscus tear\")\n",
    "    if findings:\n",
    "        return \" and a \".join(findings) + \".\"\n",
    "    else:\n",
    "        return \"Unspecified abnormality.\"\n",
    "\n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network model that combines CLIP embeddings with GPT-2 for image caption generation.\n",
    "\n",
    "    The model projects the CLIP image embeddings into a prefix representation that is then concatenated\n",
    "    with the input captions. It uses the GPT-2 model to generate captions conditioned on the image embeddings.\n",
    "\n",
    "    Args:\n",
    "        clip_dim (int, optional): The dimensionality of the CLIP image embeddings. Default is 512.\n",
    "        prefix_len (int, optional): The number of tokens in the prefix generated from the image embedding.\n",
    "                                    This prefix is concatenated with the input captions. Default is 10.\n",
    "\n",
    "    Forward Method:\n",
    "        The forward pass takes the CLIP image embedding, the caption tokens, and the attention mask as inputs.\n",
    "        It generates the image-conditioned caption by combining the image embedding prefix with the caption\n",
    "        embeddings and feeding them through the GPT-2 model.\n",
    "\n",
    "    Args:\n",
    "        image_embedding (torch.Tensor): A tensor of shape (batch_size, clip_dim) representing the image embeddings from CLIP.\n",
    "        captions (torch.Tensor): A tensor of shape (batch_size, caption_len) representing the tokenized captions.\n",
    "        attention_mask (torch.Tensor): A tensor of shape (batch_size, caption_len) indicating the padding positions in captions.\n",
    "\n",
    "    Returns:\n",
    "        transformers.modeling_outputs.CausalLMOutputWithCrossAttentions: The output of the GPT-2 model,\n",
    "        containing the logits and additional information.\n",
    "    \"\"\"\n",
    "    def __init__(self, clip_dim=512, prefix_len=10):\n",
    "        super().__init__()\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        self.prefix_len = prefix_len\n",
    "        self.clip_project = nn.Linear(clip_dim, self.gpt.config.n_embd * prefix_len)\n",
    "\n",
    "    def forward(self, image_embedding, captions, attention_mask):\n",
    "        batch_size = captions.shape[0]\n",
    "\n",
    "        image_embedding = image_embedding.float()\n",
    "\n",
    "        prefix_embedding = self.clip_project(image_embedding).view(batch_size, self.prefix_len, -1)\n",
    "        caption_embeddings = self.gpt.transformer.wte(captions)\n",
    "\n",
    "        embeddings = torch.cat((prefix_embedding, caption_embeddings), dim=1)\n",
    "\n",
    "        extended_attention = torch.cat((\n",
    "            torch.ones((batch_size, self.prefix_len), device=attention_mask.device),\n",
    "            attention_mask\n",
    "        ), dim=1)\n",
    "\n",
    "        labels = torch.cat((\n",
    "            torch.full((batch_size, self.prefix_len), -100, device=captions.device),\n",
    "            captions\n",
    "        ), dim=1)\n",
    "\n",
    "        outputs = self.gpt(inputs_embeds=embeddings, attention_mask=extended_attention, labels=labels)\n",
    "        return outputs\n",
    "\n",
    "class MRICaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset for loading MRI scans and their corresponding captions, suitable for image captioning tasks.\n",
    "\n",
    "    This dataset assumes the MRI scans are stored as numpy arrays in a specified directory, with each array representing\n",
    "    a 3D volume of slices. For each MRI scan, the dataset loads a representative slice, preprocesses it, and tokenizes\n",
    "    the associated caption. The caption is tokenized and padded/truncated to a specified length.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): A dataframe containing the metadata for the MRI scans. The dataframe should\n",
    "                                      contain columns 'exam' (MRI scan identifier) and 'caption' (text description of the scan).\n",
    "        image_dir (str): The directory containing the MRI scan files (in `.npy` format). Each file is named by its exam ID.\n",
    "        transform (callable): A transformation function to be applied to the image (e.g., resizing, normalization).\n",
    "        tokenizer (transformers.PreTrainedTokenizer): A tokenizer to tokenize the captions, typically from HuggingFace's `transformers` library.\n",
    "        max_length (int, optional): The maximum length for tokenizing captions. Default is 50 tokens.\n",
    "        num_slices_to_use (int, optional): The number of slices to select from the MRI scan volume. Default is 5 slices.\n",
    "\n",
    "    Attributes:\n",
    "        clip_dim (int): The dimension of the CLIP embedding (set to 512).\n",
    "\n",
    "    Methods:\n",
    "        __len__(): Returns the number of items in the dataset.\n",
    "        __getitem__(idx): Loads the MRI scan and its corresponding caption at the specified index `idx`, preprocesses the slice,\n",
    "                          and returns the image tensor, tokenized caption (input IDs and attention mask), the exam ID, and the selected slice indices.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - img_tensor (torch.Tensor): The transformed image tensor for the representative slice.\n",
    "            - input_ids (torch.Tensor): The tokenized caption's input IDs.\n",
    "            - attention_mask (torch.Tensor): The attention mask for the tokenized caption.\n",
    "            - exam_id (str): The MRI scan's exam ID.\n",
    "            - selected_slices (list): A list of the indices of the selected slices from the MRI scan.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, image_dir, transform, tokenizer, max_length=50, num_slices_to_use=5):\n",
    "        self.data = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.num_slices_to_use = num_slices_to_use\n",
    "        self.clip_dim = 512\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        exam_id = str(row['exam']).zfill(4)\n",
    "        caption = row['caption']\n",
    "        img_path = os.path.join(self.image_dir, f\"{exam_id}.npy\")\n",
    "\n",
    "        scan = np.load(img_path)\n",
    "        num_slices = scan.shape[0]\n",
    "\n",
    "        # select evenly spaced slices\n",
    "        if num_slices <= self.num_slices_to_use:\n",
    "            selected_slices = range(num_slices)\n",
    "        else:\n",
    "            selected_slices = np.linspace(0, num_slices-1, self.num_slices_to_use, dtype=int)\n",
    "\n",
    "        # take the middle slice for dataset output (representative slice)\n",
    "        mid_slice = num_slices // 2\n",
    "        slice_img = scan[mid_slice]\n",
    "        slice_img = ((slice_img - slice_img.min()) / (slice_img.max() - slice_img.min()) * 255).astype(np.uint8)\n",
    "        slice_rgb = np.stack([slice_img, slice_img, slice_img], axis=-1)\n",
    "        pil_img = Image.fromarray(slice_rgb)\n",
    "        img_tensor = self.transform(pil_img)\n",
    "\n",
    "        tokens = self.tokenizer(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokens.input_ids.squeeze(0)\n",
    "        attention_mask = tokens.attention_mask.squeeze(0)\n",
    "\n",
    "        return img_tensor, input_ids, attention_mask, exam_id, selected_slices.tolist() if hasattr(selected_slices, 'tolist') else list(selected_slices)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    A collate function to combine a list of samples into a batch for the DataLoader.\n",
    "\n",
    "    This function takes a batch of data, which is a list of tuples returned by the dataset's `__getitem__` method.\n",
    "    It stacks the image tensors, tokenized input IDs, and attention masks along the batch dimension to form a single\n",
    "    batch tensor. The exam IDs and selected slice indices are returned as lists without modification.\n",
    "\n",
    "    Args:\n",
    "        batch (list of tuples): A list where each element is a tuple containing:\n",
    "            - images (torch.Tensor): The transformed image tensors for each sample.\n",
    "            - input_ids (torch.Tensor): The tokenized captions' input IDs.\n",
    "            - attention_masks (torch.Tensor): The attention masks for the tokenized captions.\n",
    "            - exam_ids (str): The MRI scan exam ID.\n",
    "            - selected_slices (list): A list of indices for the selected slices in the MRI scan.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - images_tensor (torch.Tensor): A tensor of shape (batch_size, C, H, W) representing the stacked image tensors.\n",
    "            - input_ids_tensor (torch.Tensor): A tensor of shape (batch_size, max_length) representing the stacked input IDs.\n",
    "            - attention_masks_tensor (torch.Tensor): A tensor of shape (batch_size, max_length) representing the stacked attention masks.\n",
    "            - exam_ids (tuple): A tuple of exam IDs for each sample in the batch.\n",
    "            - selected_slices (tuple): A tuple of lists containing the selected slice indices for each sample in the batch.\n",
    "    \"\"\"\n",
    "    images, input_ids, attention_masks, exam_ids, selected_slices = zip(*batch)\n",
    "    images_tensor = torch.stack(images)\n",
    "    input_ids_tensor = torch.stack(input_ids)\n",
    "    attention_masks_tensor = torch.stack(attention_masks)\n",
    "\n",
    "    return images_tensor, input_ids_tensor, attention_masks_tensor, exam_ids, selected_slices\n",
    "\n",
    "def encode_selected_slices(clip_model, exam_id, selected_slices, image_dir, transform, device):\n",
    "    \"\"\"\n",
    "    Encodes selected MRI scan slices using a CLIP model and returns the mean embedding of the selected slices.\n",
    "\n",
    "    This function loads an MRI scan for a given `exam_id`, selects specific slices based on the provided indices,\n",
    "    preprocesses each slice to a 3-channel image, and computes their embeddings using the provided CLIP model.\n",
    "    It then averages the embeddings of the selected slices to produce a single mean embedding representing the scan.\n",
    "\n",
    "    Args:\n",
    "        clip_model (CLIPModel): A pre-trained CLIP model for encoding images into embeddings.\n",
    "        exam_id (str): The exam ID (MRI scan identifier) to locate the corresponding scan in the `image_dir`.\n",
    "        selected_slices (list of int): A list of indices specifying which slices of the scan to use for embedding.\n",
    "        image_dir (str): The directory containing the MRI scan files, which are expected to be in `.npy` format.\n",
    "        transform (callable): A transformation function to preprocess the slices (e.g., resizing, normalization).\n",
    "        device (torch.device): The device (CPU or GPU) on which the CLIP model and the tensors should be processed.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor representing the mean embedding of the selected MRI slices. The tensor shape is (1, embedding_dim).\n",
    "    \"\"\"\n",
    "    img_path = os.path.join(image_dir, f\"{exam_id}.npy\")\n",
    "    scan = np.load(img_path)\n",
    "\n",
    "    slice_embeddings = []\n",
    "    for slice_idx in selected_slices:\n",
    "        # convert slice to 3-channel image\n",
    "        slice_img = scan[slice_idx]\n",
    "        slice_img = ((slice_img - slice_img.min()) / (slice_img.max() - slice_img.min()) * 255).astype(np.uint8)\n",
    "        slice_rgb = np.stack([slice_img, slice_img, slice_img], axis=-1)\n",
    "        pil_img = Image.fromarray(slice_rgb)\n",
    "        img_tensor = transform(pil_img).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.encode_image(img_tensor)\n",
    "        slice_embeddings.append(embedding)\n",
    "\n",
    "    all_embeddings = torch.cat(slice_embeddings, dim=0)\n",
    "    mean_embedding = torch.mean(all_embeddings, dim=0, keepdim=True)\n",
    "\n",
    "    return mean_embedding\n",
    "\n",
    "def train_epoch(caption_model, clip_model, dataloader, optimizer, device, image_dir, transform):\n",
    "    \"\"\"\n",
    "    Trains the captioning model for one epoch on the provided dataloader.\n",
    "\n",
    "    This function runs through one full pass of the dataset, computing the embeddings for each MRI scan slice using\n",
    "    the provided CLIP model, and then computes the loss and updates the parameters of the captioning model.\n",
    "\n",
    "    Args:\n",
    "        caption_model (nn.Module): The captioning model to be trained (e.g., a model combining CLIP and GPT-2).\n",
    "        clip_model (CLIPModel): A pre-trained CLIP model used to encode the MRI scan slices into embeddings.\n",
    "        dataloader (DataLoader): A PyTorch DataLoader that provides batches of data for training.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used for model parameter updates (e.g., Adam).\n",
    "        device (torch.device): The device (CPU or GPU) where the models and tensors are processed.\n",
    "        image_dir (str): The directory containing the MRI scan files in `.npy` format.\n",
    "        transform (callable): A transformation function applied to each MRI slice before encoding.\n",
    "\n",
    "    Returns:\n",
    "        float: The average loss over the entire epoch.\n",
    "    \"\"\"\n",
    "    caption_model.train()\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        images, input_ids, attention_mask, exam_ids, selected_slices = batch\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "\n",
    "        batch_embeddings = []\n",
    "        for i in range(len(exam_ids)):\n",
    "            exam_embedding = encode_selected_slices(\n",
    "                clip_model,\n",
    "                exam_ids[i],\n",
    "                selected_slices[i],\n",
    "                image_dir,\n",
    "                transform,\n",
    "                device\n",
    "            )\n",
    "            batch_embeddings.append(exam_embedding)\n",
    "\n",
    "        image_embeddings = torch.cat(batch_embeddings, dim=0)\n",
    "\n",
    "        outputs = caption_model(image_embeddings, input_ids, attention_mask)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(caption_model, clip_model, dataloader, device, tokenizer, image_dir, transform):\n",
    "    caption_model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    true_caption_counts = {}\n",
    "    pred_caption_counts = {}\n",
    "    caption_pairs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "            images, input_ids, attention_mask, exam_ids, selected_slices = batch\n",
    "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "\n",
    "            batch_embeddings = []\n",
    "            for i in range(len(exam_ids)):\n",
    "                exam_embedding = encode_selected_slices(\n",
    "                    clip_model,\n",
    "                    exam_ids[i],\n",
    "                    selected_slices[i],\n",
    "                    image_dir,\n",
    "                    transform,\n",
    "                    device\n",
    "                )\n",
    "                batch_embeddings.append(exam_embedding)\n",
    "\n",
    "            image_embeddings = torch.cat(batch_embeddings, dim=0)\n",
    "\n",
    "            outputs = caption_model(image_embeddings, input_ids, attention_mask)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            batch_size = input_ids.size(0)\n",
    "            for i in range(batch_size):\n",
    "                true_caption = tokenizer.decode(input_ids[i], skip_special_tokens=True)\n",
    "\n",
    "                true_caption_counts[true_caption] = true_caption_counts.get(true_caption, 0) + 1\n",
    "\n",
    "                prefix_embed = caption_model.clip_project(image_embeddings[i].float().unsqueeze(0)) \\\n",
    "                    .view(1, caption_model.prefix_len, -1)\n",
    "\n",
    "                attention_prefix = torch.ones(1, caption_model.prefix_len, device=device)\n",
    "\n",
    "                generated = caption_model.gpt.generate(\n",
    "                    inputs_embeds=prefix_embed,\n",
    "                    max_length=50,\n",
    "                    num_beams=5,\n",
    "                    early_stopping=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    attention_mask=attention_prefix\n",
    "                )\n",
    "                pred_caption = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "                pred_caption_counts[pred_caption] = pred_caption_counts.get(pred_caption, 0) + 1\n",
    "\n",
    "                caption_pairs.append((true_caption, pred_caption))\n",
    "\n",
    "    # classification metrics\n",
    "    classification_metrics = calculate_classification_metrics(caption_pairs)\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(dataloader))\n",
    "\n",
    "    total_samples = len(caption_pairs)\n",
    "    unique_pred_captions = len(pred_caption_counts)\n",
    "    unique_true_captions = len(true_caption_counts)\n",
    "\n",
    "    pred_to_true_map = {}\n",
    "    for true_cap, pred_cap in caption_pairs:\n",
    "        if pred_cap not in pred_to_true_map:\n",
    "            pred_to_true_map[pred_cap] = {}\n",
    "        pred_to_true_map[pred_cap][true_cap] = pred_to_true_map[pred_cap].get(true_cap, 0) + 1\n",
    "\n",
    "    sorted_pred_captions = sorted(pred_caption_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    sorted_true_captions = sorted(true_caption_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    caption_analysis = {\n",
    "        \"total_samples\": total_samples,\n",
    "        \"unique_predicted_captions\": unique_pred_captions,\n",
    "        \"unique_true_captions\": unique_true_captions,\n",
    "        \"top_predicted_captions\": sorted_pred_captions[:10],\n",
    "        \"top_true_captions\": sorted_true_captions[:10],\n",
    "        \"prediction_to_true_map\": pred_to_true_map,\n",
    "        \"repetition_rate\": 1 - (unique_pred_captions / total_samples)\n",
    "    }\n",
    "\n",
    "    print(\"=== BASIC METRICS ===\")\n",
    "    for key, value in caption_analysis.items():\n",
    "        if key not in [\"prediction_to_true_map\"]:\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "    print(f\"\\nAverage Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"\\n=== CLASSIFICATION METRICS ===\")\n",
    "    print(f\"Overall Accuracy: {classification_metrics['overall_accuracy']:.4f}\")\n",
    "    print(f\"Macro Average Precision: {classification_metrics['macro_precision']:.4f}\")\n",
    "    print(f\"Macro Average Recall: {classification_metrics['macro_recall']:.4f}\")\n",
    "    print(f\"Macro Average F1: {classification_metrics['macro_f1']:.4f}\")\n",
    "\n",
    "    return avg_loss, classification_metrics\n",
    "\n",
    "\n",
    "def calculate_classification_metrics(caption_pairs):\n",
    "    \"\"\"\n",
    "    Calculate per-caption classification metrics treating each unique caption as a class.\n",
    "\n",
    "    Args:\n",
    "        caption_pairs: List of tuples (true_caption, predicted_caption)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing overall accuracy, macro averages, and per-caption metrics\n",
    "    \"\"\"\n",
    "    # get all unique captions\n",
    "    all_captions = set()\n",
    "    for true_cap, pred_cap in caption_pairs:\n",
    "        all_captions.add(true_cap)\n",
    "        all_captions.add(pred_cap)\n",
    "\n",
    "    per_caption_metrics = {}\n",
    "\n",
    "    for caption in all_captions:\n",
    "        # for each caption as a \"positive\" class\n",
    "        tp = sum(1 for true_cap, pred_cap in caption_pairs if true_cap == caption and pred_cap == caption)\n",
    "        fp = sum(1 for true_cap, pred_cap in caption_pairs if true_cap != caption and pred_cap == caption)\n",
    "        fn = sum(1 for true_cap, pred_cap in caption_pairs if true_cap == caption and pred_cap != caption)\n",
    "        tn = sum(1 for true_cap, pred_cap in caption_pairs if true_cap != caption and pred_cap != caption)\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        accuracy = (tp + tn) / len(caption_pairs) if len(caption_pairs) > 0 else 0.0\n",
    "\n",
    "        per_caption_metrics[caption] = {\n",
    "            'tp': tp,\n",
    "            'fp': fp,\n",
    "            'fn': fn,\n",
    "            'tn': tn,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'accuracy': accuracy,\n",
    "            'support': tp + fn\n",
    "        }\n",
    "\n",
    "    #macro averages\n",
    "    macro_precision = sum(metrics['precision'] for metrics in per_caption_metrics.values()) / len(per_caption_metrics)\n",
    "    macro_recall = sum(metrics['recall'] for metrics in per_caption_metrics.values()) / len(per_caption_metrics)\n",
    "    macro_f1 = sum(metrics['f1'] for metrics in per_caption_metrics.values()) / len(per_caption_metrics)\n",
    "\n",
    "    #accuracy\n",
    "    correct_predictions = sum(1 for true_cap, pred_cap in caption_pairs if true_cap == pred_cap)\n",
    "    overall_accuracy = correct_predictions / len(caption_pairs) if len(caption_pairs) > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        'per_caption_metrics': per_caption_metrics,\n",
    "        'macro_precision': macro_precision,\n",
    "        'macro_recall': macro_recall,\n",
    "        'macro_f1': macro_f1,\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'total_caption_classes': len(all_captions)\n",
    "    }\n",
    "\n",
    "\n",
    "def init_model(device):\n",
    "    \"\"\"\n",
    "    Initializes and returns the components required for training or inference, including a CLIP model,\n",
    "    GPT-2 tokenizer, a captioning model, and an optimizer.\n",
    "\n",
    "    This function loads the pre-trained CLIP model (ViT-B/32), freezes its parameters, and loads the GPT-2 tokenizer.\n",
    "    It also initializes the `ClipCaptionModel` for generating captions based on CLIP embeddings and sets up the\n",
    "    optimizer for the caption model.\n",
    "\n",
    "    Args:\n",
    "        device (torch.device): The device (CPU or GPU) to load the models and tensors onto.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - clip_model (CLIPModel): The pre-trained CLIP model used for image embedding extraction.\n",
    "            - preprocess (callable): The preprocessing function associated with the CLIP model.\n",
    "            - tokenizer (GPT2Tokenizer): The tokenizer for the GPT-2 model used for caption tokenization.\n",
    "            - caption_model (ClipCaptionModel): The model that generates captions from image embeddings.\n",
    "            - optimizer (torch.optim.AdamW): The optimizer for training the caption model.\n",
    "    \"\"\"\n",
    "    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "    for param in clip_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    caption_model = ClipCaptionModel(clip_dim=512, prefix_len=10).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        caption_model.parameters(),\n",
    "        lr=5e-5,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "\n",
    "    return clip_model, preprocess, tokenizer, caption_model, optimizer\n",
    "\n",
    "def train_model(train_df, val_df, num_epochs=25, batch_size=8):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    clip_model, preprocess, tokenizer, caption_model, optimizer = init_model(device)\n",
    "\n",
    "    train_dataset = MRICaptionDataset(train_df, image_dir, preprocess, tokenizer, num_slices_to_use=10)\n",
    "    val_dataset = MRICaptionDataset(val_df, val_image_dir, preprocess, tokenizer, num_slices_to_use=10)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "    best_val_loss = 100\n",
    "    metrics = {'macro_precision': [],\n",
    "        'macro_recall': [],\n",
    "        'macro_f1': [],\n",
    "        'overall_accuracy':[],\n",
    "        'train_losses': [],\n",
    "        'val_losses': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        avg_train_loss = train_epoch(caption_model, clip_model, train_loader, optimizer, device, image_dir, preprocess)\n",
    "        avg_val_loss, classification_metrics = evaluate(\n",
    "            caption_model, clip_model, val_loader, device, tokenizer, val_image_dir, preprocess\n",
    "        )\n",
    "\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(caption_model.state_dict(), f\"/content/drive/MyDrive/biodata Project/best_model_sagittal.pt\")\n",
    "\n",
    "        metrics['macro_precision'].append(classification_metrics['macro_precision'])\n",
    "        metrics['macro_recall'].append(classification_metrics['macro_recall'])\n",
    "        metrics['macro_f1'].append(classification_metrics['macro_f1'])\n",
    "        metrics['overall_accuracy'].append(classification_metrics['overall_accuracy'])\n",
    "        metrics['train_losses'].append(avg_train_loss)\n",
    "        metrics['val_losses'].append(avg_val_loss)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/content/drive/MyDrive/biodata Project/MRNet-v1.0\"\n",
    "plane = \"axial\"  # can be 'axial', 'coronal', 'sagittal'\n",
    "image_dir = os.path.join(base_dir, \"train\", plane)\n",
    "val_image_dir = os.path.join(base_dir, \"valid\", plane)\n",
    "\n",
    "\n",
    "abnormal_df_train = pd.read_csv(os.path.join(base_dir, \"train-abnormal.csv\"))\n",
    "acl_df_train = pd.read_csv(os.path.join(base_dir, \"train-acl.csv\"))\n",
    "meniscus_df_train = pd.read_csv(os.path.join(base_dir, \"train-meniscus.csv\"))\n",
    "\n",
    "abnormal_df_train.columns = ['exam', 'abnormal']\n",
    "acl_df_train.columns = ['exam', 'acl']\n",
    "meniscus_df_train.columns = ['exam', 'meniscus']\n",
    "\n",
    "train_df = abnormal_df_train.merge(acl_df_train, on='exam').merge(meniscus_df_train, on='exam')\n",
    "\n",
    "\n",
    "train_df['caption'] = train_df.apply(generate_caption, axis=1)\n",
    "\n",
    "\n",
    "acl_meniscus_mask = (train_df['acl'] == 1) & (train_df['meniscus'] == 1)\n",
    "acl_only_mask     = (train_df['acl'] == 1) & (train_df['meniscus'] == 0)\n",
    "meniscus_only_mask = (train_df['acl'] == 0) & (train_df['meniscus'] == 1)\n",
    "healthy_mask      = (train_df['abnormal'] == 0)\n",
    "unspecified_mask  = (train_df['abnormal'] == 1) & (train_df['acl'] == 0) & (train_df['meniscus'] == 0)\n",
    "\n",
    "# sample 83 from each group\n",
    "df_acl_meniscus = train_df[acl_meniscus_mask].sample(n=83, random_state=42)\n",
    "df_acl_only     = train_df[acl_only_mask].sample(n=83, random_state=42)\n",
    "df_meniscus     = train_df[meniscus_only_mask].sample(n=83, random_state=42)\n",
    "df_healthy      = train_df[healthy_mask].sample(n=83, random_state=42)\n",
    "df_unspecified  = train_df[unspecified_mask].sample(n=83, random_state=42)\n",
    "\n",
    "train_df_balanced = pd.concat([\n",
    "    df_acl_meniscus,\n",
    "    df_acl_only,\n",
    "    df_meniscus,\n",
    "    df_healthy,\n",
    "    df_unspecified\n",
    "], ignore_index=True).sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "clip_preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                         std=(0.26862954, 0.26130258, 0.27577711))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_df_val = pd.read_csv(os.path.join(base_dir, \"valid-abnormal.csv\"))\n",
    "acl_df_val = pd.read_csv(os.path.join(base_dir, \"valid-acl.csv\"))\n",
    "meniscus_df_val = pd.read_csv(os.path.join(base_dir, \"valid-meniscus.csv\"))\n",
    "\n",
    "abnormal_df_val.columns = ['exam', 'abnormal']\n",
    "acl_df_val.columns = ['exam', 'acl']\n",
    "meniscus_df_val.columns = ['exam', 'meniscus']\n",
    "\n",
    "val_df = abnormal_df_val.merge(acl_df_val, on='exam').merge(meniscus_df_val, on='exam')\n",
    "\n",
    "val_df['caption'] = train_df.apply(generate_caption, axis=1)\n",
    "\n",
    "val_df[\"caption\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = train_model(train_df_balanced, val_df, num_epochs=25, batch_size=8)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

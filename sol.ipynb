{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install nltk rouge-score\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt', force=True)\n",
    "\n",
    "\n",
    "# âœ… OPTIONAL: Check paths\n",
    "print(nltk.data.path)\n",
    "\n",
    "\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import clip\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_caption(row):\n",
    "    if row['abnormal'] == 0:\n",
    "        return \"Healthy knee\"\n",
    "\n",
    "    findings = []\n",
    "    if row['acl'] == 1:\n",
    "        findings.append(\"ACL tear\")\n",
    "    if row['meniscus'] == 1:\n",
    "        findings.append(\"Meniscus tear\")\n",
    "    if findings:\n",
    "        return \" and a \".join(findings) + \".\"\n",
    "    else:\n",
    "        return \"Unspecified abnormality.\"\n",
    "\n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network model that combines CLIP embeddings with GPT-2 for image caption generation.\n",
    "\n",
    "    The model projects the CLIP image embeddings into a prefix representation that is then concatenated\n",
    "    with the input captions. It uses the GPT-2 model to generate captions conditioned on the image embeddings.\n",
    "\n",
    "    Args:\n",
    "        clip_dim (int, optional): The dimensionality of the CLIP image embeddings. Default is 512.\n",
    "        prefix_len (int, optional): The number of tokens in the prefix generated from the image embedding.\n",
    "                                    This prefix is concatenated with the input captions. Default is 10.\n",
    "\n",
    "    Forward Method:\n",
    "        The forward pass takes the CLIP image embedding, the caption tokens, and the attention mask as inputs.\n",
    "        It generates the image-conditioned caption by combining the image embedding prefix with the caption\n",
    "        embeddings and feeding them through the GPT-2 model.\n",
    "\n",
    "    Args:\n",
    "        image_embedding (torch.Tensor): A tensor of shape (batch_size, clip_dim) representing the image embeddings from CLIP.\n",
    "        captions (torch.Tensor): A tensor of shape (batch_size, caption_len) representing the tokenized captions.\n",
    "        attention_mask (torch.Tensor): A tensor of shape (batch_size, caption_len) indicating the padding positions in captions.\n",
    "\n",
    "    Returns:\n",
    "        transformers.modeling_outputs.CausalLMOutputWithCrossAttentions: The output of the GPT-2 model,\n",
    "        containing the logits and additional information.\n",
    "    \"\"\"\n",
    "    def __init__(self, clip_dim=512, prefix_len=10):\n",
    "        super().__init__()\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        self.prefix_len = prefix_len\n",
    "        self.clip_project = nn.Linear(clip_dim, self.gpt.config.n_embd * prefix_len)\n",
    "\n",
    "    def forward(self, image_embedding, captions, attention_mask):\n",
    "        batch_size = captions.shape[0]\n",
    "\n",
    "        image_embedding = image_embedding.float()\n",
    "\n",
    "        prefix_embedding = self.clip_project(image_embedding).view(batch_size, self.prefix_len, -1)\n",
    "        caption_embeddings = self.gpt.transformer.wte(captions)\n",
    "\n",
    "        embeddings = torch.cat((prefix_embedding, caption_embeddings), dim=1)\n",
    "\n",
    "        extended_attention = torch.cat((\n",
    "            torch.ones((batch_size, self.prefix_len), device=attention_mask.device),\n",
    "            attention_mask\n",
    "        ), dim=1)\n",
    "\n",
    "        labels = torch.cat((\n",
    "            torch.full((batch_size, self.prefix_len), -100, device=captions.device),\n",
    "            captions\n",
    "        ), dim=1)\n",
    "\n",
    "        outputs = self.gpt(inputs_embeds=embeddings, attention_mask=extended_attention, labels=labels)\n",
    "        return outputs\n",
    "\n",
    "class MRICaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset for loading MRI scans and their corresponding captions, suitable for image captioning tasks.\n",
    "\n",
    "    This dataset assumes the MRI scans are stored as numpy arrays in a specified directory, with each array representing\n",
    "    a 3D volume of slices. For each MRI scan, the dataset loads a representative slice, preprocesses it, and tokenizes\n",
    "    the associated caption. The caption is tokenized and padded/truncated to a specified length.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): A dataframe containing the metadata for the MRI scans. The dataframe should\n",
    "                                      contain columns 'exam' (MRI scan identifier) and 'caption' (text description of the scan).\n",
    "        image_dir (str): The directory containing the MRI scan files (in `.npy` format). Each file is named by its exam ID.\n",
    "        transform (callable): A transformation function to be applied to the image (e.g., resizing, normalization).\n",
    "        tokenizer (transformers.PreTrainedTokenizer): A tokenizer to tokenize the captions, typically from HuggingFace's `transformers` library.\n",
    "        max_length (int, optional): The maximum length for tokenizing captions. Default is 50 tokens.\n",
    "        num_slices_to_use (int, optional): The number of slices to select from the MRI scan volume. Default is 5 slices.\n",
    "\n",
    "    Attributes:\n",
    "        clip_dim (int): The dimension of the CLIP embedding (set to 512).\n",
    "\n",
    "    Methods:\n",
    "        __len__(): Returns the number of items in the dataset.\n",
    "        __getitem__(idx): Loads the MRI scan and its corresponding caption at the specified index `idx`, preprocesses the slice,\n",
    "                          and returns the image tensor, tokenized caption (input IDs and attention mask), the exam ID, and the selected slice indices.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - img_tensor (torch.Tensor): The transformed image tensor for the representative slice.\n",
    "            - input_ids (torch.Tensor): The tokenized caption's input IDs.\n",
    "            - attention_mask (torch.Tensor): The attention mask for the tokenized caption.\n",
    "            - exam_id (str): The MRI scan's exam ID.\n",
    "            - selected_slices (list): A list of the indices of the selected slices from the MRI scan.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, image_dir, transform, tokenizer, max_length=50, num_slices_to_use=5):\n",
    "        self.data = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.num_slices_to_use = num_slices_to_use\n",
    "        self.clip_dim = 512 \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        exam_id = str(row['exam']).zfill(4)\n",
    "        caption = row['caption']\n",
    "        img_path = os.path.join(self.image_dir, f\"{exam_id}.npy\")\n",
    "\n",
    "        scan = np.load(img_path)  \n",
    "        num_slices = scan.shape[0]\n",
    "        \n",
    "        # select evenly spaced slices\n",
    "        if num_slices <= self.num_slices_to_use:\n",
    "            selected_slices = range(num_slices)\n",
    "        else:\n",
    "            selected_slices = np.linspace(0, num_slices-1, self.num_slices_to_use, dtype=int)\n",
    "            \n",
    "        # take the middle slice for dataset output (representative slice)\n",
    "        mid_slice = num_slices // 2\n",
    "        slice_img = scan[mid_slice]\n",
    "        slice_img = ((slice_img - slice_img.min()) / (slice_img.max() - slice_img.min()) * 255).astype(np.uint8)\n",
    "        slice_rgb = np.stack([slice_img, slice_img, slice_img], axis=-1)\n",
    "        pil_img = Image.fromarray(slice_rgb)\n",
    "        img_tensor = self.transform(pil_img)\n",
    "\n",
    "        tokens = self.tokenizer(\n",
    "            caption, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True,\n",
    "            max_length=self.max_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokens.input_ids.squeeze(0)\n",
    "        attention_mask = tokens.attention_mask.squeeze(0)\n",
    "\n",
    "        return img_tensor, input_ids, attention_mask, exam_id, selected_slices.tolist() if hasattr(selected_slices, 'tolist') else list(selected_slices)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    A collate function to combine a list of samples into a batch for the DataLoader.\n",
    "\n",
    "    This function takes a batch of data, which is a list of tuples returned by the dataset's `__getitem__` method.\n",
    "    It stacks the image tensors, tokenized input IDs, and attention masks along the batch dimension to form a single\n",
    "    batch tensor. The exam IDs and selected slice indices are returned as lists without modification.\n",
    "\n",
    "    Args:\n",
    "        batch (list of tuples): A list where each element is a tuple containing:\n",
    "            - images (torch.Tensor): The transformed image tensors for each sample.\n",
    "            - input_ids (torch.Tensor): The tokenized captions' input IDs.\n",
    "            - attention_masks (torch.Tensor): The attention masks for the tokenized captions.\n",
    "            - exam_ids (str): The MRI scan exam ID.\n",
    "            - selected_slices (list): A list of indices for the selected slices in the MRI scan.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - images_tensor (torch.Tensor): A tensor of shape (batch_size, C, H, W) representing the stacked image tensors.\n",
    "            - input_ids_tensor (torch.Tensor): A tensor of shape (batch_size, max_length) representing the stacked input IDs.\n",
    "            - attention_masks_tensor (torch.Tensor): A tensor of shape (batch_size, max_length) representing the stacked attention masks.\n",
    "            - exam_ids (tuple): A tuple of exam IDs for each sample in the batch.\n",
    "            - selected_slices (tuple): A tuple of lists containing the selected slice indices for each sample in the batch.\n",
    "    \"\"\"\n",
    "    images, input_ids, attention_masks, exam_ids, selected_slices = zip(*batch)\n",
    "    images_tensor = torch.stack(images)\n",
    "    input_ids_tensor = torch.stack(input_ids)\n",
    "    attention_masks_tensor = torch.stack(attention_masks)\n",
    "    \n",
    "    return images_tensor, input_ids_tensor, attention_masks_tensor, exam_ids, selected_slices\n",
    "\n",
    "def encode_selected_slices(clip_model, exam_id, selected_slices, image_dir, transform, device):\n",
    "    \"\"\"\n",
    "    Encodes selected MRI scan slices using a CLIP model and returns the mean embedding of the selected slices.\n",
    "\n",
    "    This function loads an MRI scan for a given `exam_id`, selects specific slices based on the provided indices,\n",
    "    preprocesses each slice to a 3-channel image, and computes their embeddings using the provided CLIP model.\n",
    "    It then averages the embeddings of the selected slices to produce a single mean embedding representing the scan.\n",
    "\n",
    "    Args:\n",
    "        clip_model (CLIPModel): A pre-trained CLIP model for encoding images into embeddings.\n",
    "        exam_id (str): The exam ID (MRI scan identifier) to locate the corresponding scan in the `image_dir`.\n",
    "        selected_slices (list of int): A list of indices specifying which slices of the scan to use for embedding.\n",
    "        image_dir (str): The directory containing the MRI scan files, which are expected to be in `.npy` format.\n",
    "        transform (callable): A transformation function to preprocess the slices (e.g., resizing, normalization).\n",
    "        device (torch.device): The device (CPU or GPU) on which the CLIP model and the tensors should be processed.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor representing the mean embedding of the selected MRI slices. The tensor shape is (1, embedding_dim).\n",
    "    \"\"\"\n",
    "    img_path = os.path.join(image_dir, f\"{exam_id}.npy\")\n",
    "    scan = np.load(img_path)\n",
    "    \n",
    "    slice_embeddings = []\n",
    "    for slice_idx in selected_slices:\n",
    "        # convert slice to 3-channel image\n",
    "        slice_img = scan[slice_idx]\n",
    "        slice_img = ((slice_img - slice_img.min()) / (slice_img.max() - slice_img.min()) * 255).astype(np.uint8)\n",
    "        slice_rgb = np.stack([slice_img, slice_img, slice_img], axis=-1)\n",
    "        pil_img = Image.fromarray(slice_rgb)\n",
    "        img_tensor = transform(pil_img).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.encode_image(img_tensor)\n",
    "        slice_embeddings.append(embedding)\n",
    "    \n",
    "    all_embeddings = torch.cat(slice_embeddings, dim=0)\n",
    "    mean_embedding = torch.mean(all_embeddings, dim=0, keepdim=True)\n",
    "    \n",
    "    return mean_embedding\n",
    "\n",
    "def train_epoch(caption_model, clip_model, dataloader, optimizer, device, image_dir, transform):\n",
    "    \"\"\"\n",
    "    Trains the captioning model for one epoch on the provided dataloader.\n",
    "\n",
    "    This function runs through one full pass of the dataset, computing the embeddings for each MRI scan slice using\n",
    "    the provided CLIP model, and then computes the loss and updates the parameters of the captioning model.\n",
    "\n",
    "    Args:\n",
    "        caption_model (nn.Module): The captioning model to be trained (e.g., a model combining CLIP and GPT-2).\n",
    "        clip_model (CLIPModel): A pre-trained CLIP model used to encode the MRI scan slices into embeddings.\n",
    "        dataloader (DataLoader): A PyTorch DataLoader that provides batches of data for training.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used for model parameter updates (e.g., Adam).\n",
    "        device (torch.device): The device (CPU or GPU) where the models and tensors are processed.\n",
    "        image_dir (str): The directory containing the MRI scan files in `.npy` format.\n",
    "        transform (callable): A transformation function applied to each MRI slice before encoding.\n",
    "\n",
    "    Returns:\n",
    "        float: The average loss over the entire epoch.\n",
    "    \"\"\"\n",
    "    caption_model.train()\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        images, input_ids, attention_mask, exam_ids, selected_slices = batch\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "        \n",
    "        batch_embeddings = []\n",
    "        for i in range(len(exam_ids)):\n",
    "            exam_embedding = encode_selected_slices(\n",
    "                clip_model,\n",
    "                exam_ids[i],\n",
    "                selected_slices[i],\n",
    "                image_dir,\n",
    "                transform,\n",
    "                device\n",
    "            )\n",
    "            batch_embeddings.append(exam_embedding)\n",
    "        \n",
    "        image_embeddings = torch.cat(batch_embeddings, dim=0)\n",
    "        \n",
    "        outputs = caption_model(image_embeddings, input_ids, attention_mask)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(caption_model, clip_model, dataloader, device, tokenizer, image_dir, transform):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of the captioning model on a validation dataset.\n",
    "\n",
    "    This function evaluates the captioning model by computing the loss, BLEU score, and ROUGE score for each generated\n",
    "    caption. It also collects statistics on the true and predicted captions, such as repetition rates, and provides\n",
    "    an analysis of caption diversity and frequency.\n",
    "\n",
    "    Args:\n",
    "        caption_model (nn.Module): The captioning model (e.g., CLIP + GPT-2 model) to evaluate.\n",
    "        clip_model (CLIPModel): A pre-trained CLIP model used to encode MRI scan slices into embeddings.\n",
    "        dataloader (DataLoader): A PyTorch DataLoader providing the validation dataset in batches.\n",
    "        device (torch.device): The device (CPU or GPU) used to process the model and tensors.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): A tokenizer to decode tokenized captions into text.\n",
    "        image_dir (str): The directory containing the MRI scan files (in `.npy` format).\n",
    "        transform (callable): A transformation function applied to each MRI slice before encoding.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - avg_loss (float): The average loss over the validation set.\n",
    "            - avg_bleu (float): The average BLEU score over the validation set.\n",
    "            - avg_rouge (float): The average ROUGE score over the validation set.\n",
    "    \n",
    "    Additionally, prints an analysis of caption predictions, including:\n",
    "        - total_samples: Total number of samples in the validation set.\n",
    "        - unique_predicted_captions: The number of unique predicted captions.\n",
    "        - unique_true_captions: The number of unique true captions.\n",
    "        - top_predicted_captions: The top 10 most frequent predicted captions.\n",
    "        - top_true_captions: The top 10 most frequent ground truth captions.\n",
    "        - prediction_to_true_map: A mapping of predicted captions to corresponding true captions.\n",
    "        - repetition_rate: The rate at which the same predicted caption repeats across the validation set (higher means more repetition).\n",
    "    \"\"\"\n",
    "    caption_model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    true_caption_counts = {}  \n",
    "    pred_caption_counts = {}  \n",
    "    caption_pairs = []  \n",
    "\n",
    "    all_bleu_scores = []\n",
    "    all_rouge_scores = []\n",
    "    \n",
    "    smooth = SmoothingFunction().method4\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "            images, input_ids, attention_mask, exam_ids, selected_slices = batch\n",
    "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "\n",
    "            batch_embeddings = []\n",
    "            for i in range(len(exam_ids)):\n",
    "                exam_embedding = encode_selected_slices(\n",
    "                    clip_model,\n",
    "                    exam_ids[i],\n",
    "                    selected_slices[i],\n",
    "                    image_dir,\n",
    "                    transform,\n",
    "                    device\n",
    "                )\n",
    "                batch_embeddings.append(exam_embedding)\n",
    "            \n",
    "            image_embeddings = torch.cat(batch_embeddings, dim=0)\n",
    "\n",
    "            outputs = caption_model(image_embeddings, input_ids, attention_mask)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            batch_size = input_ids.size(0)\n",
    "            for i in range(batch_size):\n",
    "                true_caption = tokenizer.decode(input_ids[i], skip_special_tokens=True)\n",
    "                \n",
    "                if true_caption in true_caption_counts:\n",
    "                    true_caption_counts[true_caption] += 1\n",
    "                else:\n",
    "                    true_caption_counts[true_caption] = 1\n",
    "                \n",
    "                prefix_embed = caption_model.clip_project(image_embeddings[i].float().unsqueeze(0)) \\\n",
    "                    .view(1, caption_model.prefix_len, -1)\n",
    "                \n",
    "                attention_prefix = torch.ones(1, caption_model.prefix_len, device=device)\n",
    "                \n",
    "                generated = caption_model.gpt.generate(\n",
    "                    inputs_embeds=prefix_embed,\n",
    "                    max_length=50,\n",
    "                    num_beams=5,\n",
    "                    early_stopping=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    attention_mask=attention_prefix\n",
    "                )\n",
    "                pred_caption = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "                \n",
    "                if pred_caption in pred_caption_counts:\n",
    "                    pred_caption_counts[pred_caption] += 1\n",
    "                else:\n",
    "                    pred_caption_counts[pred_caption] = 1\n",
    "                \n",
    "                caption_pairs.append((true_caption, pred_caption))\n",
    "                \n",
    "                reference = [nltk.word_tokenize(true_caption.lower())]\n",
    "                candidate = nltk.word_tokenize(pred_caption.lower())\n",
    "                if len(candidate) > 0:  \n",
    "                    bleu = sentence_bleu(reference, candidate, smoothing_function=smooth)\n",
    "                    all_bleu_scores.append(bleu)\n",
    "                \n",
    "                rouge = scorer.score(true_caption, pred_caption)['rougeL'].fmeasure\n",
    "                all_rouge_scores.append(rouge)\n",
    "                    \n",
    "    avg_loss = total_loss / max(1, len(dataloader))\n",
    "    avg_bleu = sum(all_bleu_scores) / max(1, len(all_bleu_scores))\n",
    "    avg_rouge = sum(all_rouge_scores) / max(1, len(all_rouge_scores))\n",
    "\n",
    "    total_samples = len(caption_pairs)\n",
    "    unique_pred_captions = len(pred_caption_counts)\n",
    "    unique_true_captions = len(true_caption_counts)\n",
    "    \n",
    "    pred_to_true_map = {}\n",
    "    for true_cap, pred_cap in caption_pairs:\n",
    "        if pred_cap not in pred_to_true_map:\n",
    "            pred_to_true_map[pred_cap] = {}\n",
    "        \n",
    "        if true_cap not in pred_to_true_map[pred_cap]:\n",
    "            pred_to_true_map[pred_cap][true_cap] = 1\n",
    "        else:\n",
    "            pred_to_true_map[pred_cap][true_cap] += 1\n",
    "    \n",
    "    sorted_pred_captions = sorted(pred_caption_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    sorted_true_captions = sorted(true_caption_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    caption_analysis = {\n",
    "        \"total_samples\": total_samples,\n",
    "        \"unique_predicted_captions\": unique_pred_captions,\n",
    "        \"unique_true_captions\": unique_true_captions,\n",
    "        \"top_predicted_captions\": sorted_pred_captions[:10],  \n",
    "        \"top_true_captions\": sorted_true_captions[:10],       \n",
    "        \"prediction_to_true_map\": pred_to_true_map,           \n",
    "        \"repetition_rate\": 1 - (unique_pred_captions / total_samples)\n",
    "    }\n",
    "    for key, value in caption_analysis.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    return avg_loss, avg_bleu, avg_rouge\n",
    "\n",
    "def init_model(device):\n",
    "    \"\"\"\n",
    "    Initializes and returns the components required for training or inference, including a CLIP model,\n",
    "    GPT-2 tokenizer, a captioning model, and an optimizer.\n",
    "\n",
    "    This function loads the pre-trained CLIP model (ViT-B/32), freezes its parameters, and loads the GPT-2 tokenizer.\n",
    "    It also initializes the `ClipCaptionModel` for generating captions based on CLIP embeddings and sets up the\n",
    "    optimizer for the caption model.\n",
    "\n",
    "    Args:\n",
    "        device (torch.device): The device (CPU or GPU) to load the models and tensors onto.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - clip_model (CLIPModel): The pre-trained CLIP model used for image embedding extraction.\n",
    "            - preprocess (callable): The preprocessing function associated with the CLIP model.\n",
    "            - tokenizer (GPT2Tokenizer): The tokenizer for the GPT-2 model used for caption tokenization.\n",
    "            - caption_model (ClipCaptionModel): The model that generates captions from image embeddings.\n",
    "            - optimizer (torch.optim.AdamW): The optimizer for training the caption model.\n",
    "    \"\"\"\n",
    "    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    \n",
    "    for param in clip_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    caption_model = ClipCaptionModel(clip_dim=512, prefix_len=10).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        caption_model.parameters(),\n",
    "        lr=5e-5,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    return clip_model, preprocess, tokenizer, caption_model, optimizer\n",
    "\n",
    "def train_model(num_epochs=25, batch_size=8):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    clip_model, preprocess, tokenizer, caption_model, optimizer = init_model(device)\n",
    "    \n",
    "    train_df, val_df = train_test_split(df_balanced, test_size=0.1, random_state=42, stratify=df_balanced['caption'])\n",
    "    \n",
    "    train_dataset = MRICaptionDataset(train_df, image_dir, preprocess, tokenizer, num_slices_to_use=10)\n",
    "    val_dataset = MRICaptionDataset(val_df, image_dir, preprocess, tokenizer, num_slices_to_use=10)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    \n",
    "    best_rouge = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nðŸŒŸ Epoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        avg_train_loss = train_epoch(caption_model, clip_model, train_loader, optimizer, device, image_dir, preprocess)\n",
    "        avg_val_loss, avg_bleu, avg_rouge = evaluate(\n",
    "            caption_model, clip_model, val_loader, device, tokenizer, image_dir, preprocess\n",
    "        )\n",
    "        \n",
    "        print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"BLEU: {avg_bleu:.4f} | ROUGE-L: {avg_rouge:.4f}\")\n",
    "        \n",
    "        if avg_rouge > best_rouge:\n",
    "            best_rouge = avg_rouge\n",
    "            torch.save(caption_model.state_dict(), f\"/content/drive/MyDrive/biodata Project/best_model.pt\")\n",
    "\n",
    "base_dir = \"/content/drive/MyDrive/biodata Project/MRNet-v1.0\"\n",
    "plane = \"sagittal\"  # can be 'axial', 'coronal', 'sagittal'\n",
    "label_csv = os.path.join(base_dir, \"train-abnormal.csv\")\n",
    "image_dir = os.path.join(base_dir, \"train\", plane)\n",
    "\n",
    "\n",
    "abnormal_df = pd.read_csv(os.path.join(base_dir, \"train-abnormal.csv\"))\n",
    "acl_df = pd.read_csv(os.path.join(base_dir, \"train-acl.csv\"))\n",
    "meniscus_df = pd.read_csv(os.path.join(base_dir, \"train-meniscus.csv\"))\n",
    "\n",
    "abnormal_df.columns = ['exam', 'abnormal']\n",
    "acl_df.columns = ['exam', 'acl']\n",
    "meniscus_df.columns = ['exam', 'meniscus']\n",
    "\n",
    "df = abnormal_df.merge(acl_df, on='exam').merge(meniscus_df, on='exam')\n",
    "\n",
    "\n",
    "df['caption'] = df.apply(generate_caption, axis=1)\n",
    "\n",
    "\n",
    "acl_meniscus_mask = (df['acl'] == 1) & (df['meniscus'] == 1)\n",
    "acl_only_mask     = (df['acl'] == 1) & (df['meniscus'] == 0)\n",
    "meniscus_only_mask = (df['acl'] == 0) & (df['meniscus'] == 1)\n",
    "healthy_mask      = (df['abnormal'] == 0)\n",
    "unspecified_mask  = (df['abnormal'] == 1) & (df['acl'] == 0) & (df['meniscus'] == 0)\n",
    "\n",
    "# Sample 83 from each group\n",
    "df_acl_meniscus = df[acl_meniscus_mask].sample(n=83, random_state=42)\n",
    "df_acl_only     = df[acl_only_mask].sample(n=83, random_state=42)\n",
    "df_meniscus     = df[meniscus_only_mask].sample(n=83, random_state=42)\n",
    "df_healthy      = df[healthy_mask].sample(n=83, random_state=42)\n",
    "df_unspecified  = df[unspecified_mask].sample(n=83, random_state=42)\n",
    "\n",
    "# Concatenate them\n",
    "df_balanced = pd.concat([\n",
    "    df_acl_meniscus,\n",
    "    df_acl_only,\n",
    "    df_meniscus,\n",
    "    df_healthy,\n",
    "    df_unspecified\n",
    "], ignore_index=True).sample(frac=1, random_state=42)\n",
    "\n",
    "df_balanced[\"caption\"].value_counts()\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "# Use CLIP preprocessing (from OpenAI or OpenCLIP)\n",
    "clip_preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                         std=(0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "train_model(num_epochs=25, batch_size=8)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
